## Thesis

## REPOSITORY STRUCTURE

- docker-compose.yml
- [Metamorphosys](./Metamorphosys): contains code for the Metamorphosys PoC.
  - [avro](./Metamorphosys/avro): contains the AVRO schemas used.
  - [src](./Metamorphosys/src): actual Java source code and RSP-QL ANTLR grammar.
- [python](./python): contains helping python scripts.
  - [rdf_to_stream](./python/rdf_to_stream): script, queries and Dockerfile for the first component of the producing pipeline for RDF Data, already uploaded to [dockerhub](cloud.docker.com/) as `phisco/converter`
  - [kafka](./python/kafka): scripts for a producer (second component) using a specified avro schema to upload the data generated by the first component to a Kafka topic and an example consumer to read them, contains also their relative Dockerfiles and manifests to deploy them on Kubernetes.

## HOW TO RUN Locally

We use `docker-compose` to deploy and test locally. To populate the topics the container `avro_producer` has to be run and it needs every other component in the docker-compose file. We have specified such dependencies, therefore you only need to run the next command until "thesis_avro_producer_1 is up-to-date" is shown:

```
docker-compose up -d avro_producer
```

Then set the correct query at `./Metamorphosys/src/main/java/phisco/streams/polimi/it/Parser/GregorTest.java` and run it.

## SRBENCH DATA CONVERTION

These step are not necessary, beacuse the results are already embedded in the container `phisco/avro_producer`, but to regenerate them do the following:

- Download http://sonicbanana.cs.wright.edu/knoesis_linkedobservationdata_bill.tar.gz from http://wiki.knoesis.org/index.php/LinkedSensorData
- To execute the converter from the n3 files to streamable timestamped triples in json, sorted by timestamp, `{ subject: ..., predicate: ..., object: ..., timestamp: ... }` use the template in the `Makefile`, `make build-converter run-converter` to execute it into a Docker container or `make generate-data` to run it locally.

## NB

Manifests to deploy Apache Kafka as the Data Infrastructure can be found in this other [repo](https://github.com/phisco/k8s-kafka).
